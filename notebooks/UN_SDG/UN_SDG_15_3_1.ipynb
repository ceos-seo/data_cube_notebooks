{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "<a id=\"top\"></a>\n",
    "# UN SDG Indicator 15.3.1:<br> Proportion of Land That Is Degraded over Total Land Area\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Definitions\n",
    "\n",
    "> **Land degradation** is defined as the reduction or loss of the biological or economic productivity and complexity of rain fed cropland, irrigated cropland, or range, pasture, forest and woodlands resulting from a combination of pressures, including land use and management practices. This definition was adopted by and is used by the 196 countries that are Party to the UNCCD.<sup>1</sup>\n",
    "\n",
    "> **Total land area** is the total surface area of a country excluding the area covered by inland waters, like major rivers and lakes.<sup>2</sup>\n",
    "\n",
    "<sup>1</sup> United Nations Convention to Combat Desertification. 1994. Article 1 of the Convention Text http://www2.unccd.int/sites/default/files/relevant-links/2017-01/UNCCD_Convention_ENG_0.pdf\n",
    "\n",
    "<sup>2</sup> Food and Agriculture Organization of the United Nations\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Notebook Summary\n",
    "The United Nations have prescribed 17 \"Sustainable Development Goals\" (SDGs). This notebook attempts to monitor SDG Indicator 15.3.1 - the proportion of land that is degraded over total land area.\n",
    "\n",
    "There are 2 supported land classes: ESA CCI LC and FROM-GLC.\n",
    "\n",
    "The ESA CCI LC dataset has a resolution of 300 meters and is the default dataset for UN SDG 15.3.1 according to the [Good Practice Guidance document](https://www.unccd.int/sites/default/files/relevant-links/2017-10/Good%20Practice%20Guidance_SDG%20Indicator%2015.3.1_Version%201.0.pdf).\n",
    "\n",
    "The FROM-GLC dataset has a resolution of 30 meters and in our experience has been much more accurate than the ESA CCI data - irrespective of the higher resolution. Specifically, this dataset has been seen to show land class changes between 2000 and 2015 much more accurately than the ESA CCI LC dataset, which often underestimates these changes.\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Index\n",
    "\n",
    "* [Import Dependencies and Connect to the Data Cube](#import)\n",
    "* [Choose Platforms and Products](#plat_prod)\n",
    "* [Get the Extents of the Cube](#extents)\n",
    "* [Define the Extents of the Analysis](#define_extents)\n",
    "* [Load the Data](#load_data)\n",
    "    * [Load Land Classification Data](#load_land_class_data)\n",
    "    * [Load EO Data from the Datacube](#load_data_cube)\n",
    "* [Train Random Forest Classifier](#train_cls)\n",
    "    * [Train Model](#train_model)\n",
    "    * [Get Model Score and  Predictions](#get_score_pred)\n",
    "    * [Prepare for Visualization](#prepare_vis)\n",
    "    * [Visualize Land Class Data for the Beginning and End of the Time Range](#vis_ESA_CCI)\n",
    "    * [Visualize Predicted Land Classes (from Model) and \"True\" Land Classes](#vis_pred_true_land_cls)\n",
    "* [Visualize Land Change Frequency](#land_change_freq)\n",
    "* [Create Change Matrix](#change_matrix)\n",
    "* [Calculate Peak NDVI Within Classes](#peak_ndvi_within_class)\n",
    "* [Determine NDVI Trend](#ndvi_trend)\n",
    "    * [Fit a Thiel-Sen Regressor to the Mean NDVI Across Time](#thiel_sen_reg)\n",
    "    * [Run the Mann-Kendall Test on the Mean NDVI Across Time to Determine the Trend](#mann_kendall_test)\n",
    "    \n",
    "# Data  Retrieval\n",
    "\n",
    "## ESA CCI LC (300m resolution)\n",
    "The ESA CCI data can be retrieved from [this webpage](http://maps.elie.ucl.ac.be/CCI/viewer/download.php). More specifically, we need the one `.tif` file with 24 bands covering years 1992 to 2015, which should be available [here](https://storage.googleapis.com/cci-lc-v207/ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992_2015-v2.0.7.zip).\n",
    "\n",
    "Put that file in `data/ESA_CCI` directory. The `data` directory must be in the root directory of the notebook server, which should be the parent directory of this directory. Unzip it to create a `scratch` folder, which will contain the `.tif` file we need. Move that `.tif` file up into the `data/ESA_CCI` directory and delete the `scratch` folder. You may also delete the `.zip` that was downloaded once the `.tif` has been extracted.\n",
    "\n",
    "## FROM-GLC (30m resolution)\n",
    "The FROM-GLC data should be indexed by the Data Cube on this machine before running this notebook. Contact a system administrator if there is no Data Cube product containing FROM-GLC data for your area of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## <span id=\"import\">Import Dependencies and Connect to the Data Cube [&#9652;](#top)</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T15:33:46.143365Z",
     "iopub.status.busy": "2020-09-28T15:33:46.142931Z",
     "iopub.status.idle": "2020-09-28T15:33:47.284845Z",
     "shell.execute_reply": "2020-09-28T15:33:47.284333Z"
    },
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Allow importing of our utilities.\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.environ.get('NOTEBOOK_ROOT'))\n",
    "\n",
    "# Import the datacube and the API\n",
    "import datacube\n",
    "from utils.data_cube_utilities.data_access_api import DataAccessApi\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# PLotting settings\n",
    "default_fontsize = 14\n",
    "plt.rcParams['font.size'] = default_fontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T15:33:47.289028Z",
     "iopub.status.busy": "2020-09-28T15:33:47.288337Z",
     "iopub.status.idle": "2020-09-28T15:33:47.601046Z",
     "shell.execute_reply": "2020-09-28T15:33:47.601541Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Create an instance of the datacube and API.\n",
    "api = DataAccessApi()\n",
    "dc = api.dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## <span id=\"plat_prod\">Choose Platforms and Products [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "**List available products for each platform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T15:33:47.606391Z",
     "iopub.status.busy": "2020-09-28T15:33:47.605699Z",
     "iopub.status.idle": "2020-09-28T15:33:47.636772Z",
     "shell.execute_reply": "2020-09-28T15:33:47.636334Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Get available products\n",
    "products_info = dc.list_products()\n",
    "\n",
    "# List LANDSAT 7 products\n",
    "print(\"LANDSAT 7 Products:\")\n",
    "products_info[[\"platform\", \"name\"]][products_info.platform == \"LANDSAT_7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T15:33:47.644227Z",
     "iopub.status.busy": "2020-09-28T15:33:47.643696Z",
     "iopub.status.idle": "2020-09-28T15:33:47.647046Z",
     "shell.execute_reply": "2020-09-28T15:33:47.646622Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# List LANDSAT 8 products\n",
    "print(\"LANDSAT 8 Products:\")\n",
    "products_info[[\"platform\", \"name\"]][products_info.platform == \"LANDSAT_8\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "**Choose products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T15:33:47.650690Z",
     "iopub.status.busy": "2020-09-28T15:33:47.650226Z",
     "iopub.status.idle": "2020-09-28T15:33:47.652307Z",
     "shell.execute_reply": "2020-09-28T15:33:47.651880Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# These are the platforms (satellites) and products (datacube sets) \n",
    "# used for this demonstration.\n",
    "use_Landsat7 = True\n",
    "use_Landsat8 = False\n",
    "platforms = []\n",
    "products = []\n",
    "if use_Landsat7:\n",
    "    platforms.append('LANDSAT_7')\n",
    "    products.append('ls7_ledaps_ghana')\n",
    "if use_Landsat8:\n",
    "    platforms.append('LANDSAT_8')\n",
    "    products.append('ls8_lasrc_ghana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## <span id=\"extents\">Get the Extents of the Cube [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T15:33:47.655452Z",
     "iopub.status.busy": "2020-09-28T15:33:47.655030Z",
     "iopub.status.idle": "2020-09-28T15:33:47.938099Z",
     "shell.execute_reply": "2020-09-28T15:33:47.937387Z"
    },
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_display_map import display_map\n",
    "from utils.data_cube_utilities.dc_load import get_overlapping_area\n",
    "\n",
    "# Get the area common to all products.\n",
    "full_lat, full_lon, min_max_dates = get_overlapping_area(api, platforms, products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the total shared area available for these datacube products.\n",
    "display_map(latitude = full_lat,longitude = full_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## <span id=\"define_extents\">Define the Extents of the Analysis [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "**Specify start and end dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(\"Start and end dates available to both products:\")\n",
    "for platform, min_max_date in zip(platforms, min_max_dates):\n",
    "    print(\"{}:\\n{}\".format(platform, min_max_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "**Specify an area to analyze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Specify latitude and longitude bounds of an interesting area within the full extents.\n",
    "\n",
    "## Ghana\n",
    "# lat_small = (6.5016, 6.5821) # Lake Volta (WOFS Test)\n",
    "# lon_small = (-0.1618, -0.055) # Lake Volta (WOFS Test)\n",
    "# lat_small = (6.5138, 6.5292) # Lake Volta (WOFS Test - small)\n",
    "# lon_small = (-0.1669, -0.1493) # Lake Volta (WOFS Test - small)\n",
    "# lat_small = (10.0, 11.0) # NE Ghana (Land Change)\n",
    "# lon_small = (-0.9, -0.1) # NE Ghana (Land Change)\n",
    "# lat_small = (10.45, 10.55) # NE Ghana (Land Change - small)\n",
    "# lon_small = (-0.55, -0.45) # NE Ghana (Land Change - small)\n",
    "# lat_small = (10.50, 10.55) # NE Ghana (Land Change - very small)\n",
    "# lon_small = (-0.50, -0.45) # NE Ghana (Land Change - very small)\n",
    "# lat_small = (5.3180, 5.5500) # S Ghana (Land Change - #2)\n",
    "# lon_small = (-0.6505,-0.4110) # S Ghana (Land Change - #2)\n",
    "\n",
    "# lat_small = (5.4500, 5.7500) # S Ghana (Land Change - #3 - Accra)\n",
    "# lon_small = (-0.3250, 0.0000) # S Ghana (Land Change - #3 - Accra)\n",
    "\n",
    "# lat_small = (5.4500, 5.6500) # S Ghana (Land Change - #3 - Kasoa - small)\n",
    "# lon_small = (-0.5500, -0.3250) # S Ghana (Land Change - #3 - Kasoa - small)\n",
    "# lat_small = (5.2500, 6.0000) # S Ghana (Land Change - #4)\n",
    "# lon_small = (-2.0000, 1.0000) # S Ghana (Land Change - #4)\n",
    "lat_small = (6.5500, 6.8000) # S Ghana (Land Change - #3 - Kumasi - small)\n",
    "lon_small = (-1.7500, -1.5000) # S Ghana (Land Change - #3 - Kumasi - small)\n",
    "# lat_small = (6.5500, 6.6000) # S Ghana (Land Change - #3 - Kumasi - very small)\n",
    "# lon_small = (-1.6000, -1.5500) # S Ghana (Land Change - #3 - Kumasi - very small)\n",
    "\n",
    "## Vietnam\n",
    "# Can Tho\n",
    "# min_lat_small, max_lat_small = (9.9795, 10.1000)\n",
    "# min_lon_small, max_lon_small = (105.7058, 105.8058)\n",
    "# Ho Chi Minh City\n",
    "# min_lat_small, max_lat_small = (10.6452, 10.8452)\n",
    "# min_lon_small, max_lon_small = (106.5994, 106.7994)\n",
    "# Phu Thuy\n",
    "# lat_small = (10.8945, 10.9679)\n",
    "# lon_small = (108.0616, 108.1460)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "**Visualize the selected area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_display_map import display_map\n",
    "display_map(lat_small, lon_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## <span id=\"load_data\">Load the Data [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.array([2000, 2015]) # These are the years that we will load data for.\n",
    "first_year, last_year = years[0], years[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### <span id=\"load_land_class_data\">Load Land Classification Data [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESA CCI LC Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "def load_ESA_CCI_data(fp='../data/ESA_CCI/ESACCI-LC-L4-LCCS-Map-300m-P1Y-1992_2015-v2.0.7.tif',\n",
    "                      years=years, lat=lat_small, lon=lon_small):\n",
    "    \"\"\"\n",
    "    Loads ESA CCI data from a file path to a GeoTIFF. It is yearly data from 1992-2015 at 300m resolution \n",
    "    containing land classifications from the UN Land Cover Classification SYstem (LCCS) \n",
    "    (http://www.fao.org/land-water/land/land-governance/land-resources-planning-toolbox/category/details/en/c/1036361/).\n",
    "\n",
    "    The data covers the entire world with a latitude range of (-90, 90) and a longitude range of (-180, 180).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fp: str\n",
    "        The string filepath to a GeoTIFF containing 24 years of data - 1992-2015.\n",
    "    years: list of int\n",
    "        The years to retrieve data for.\n",
    "    lat, lon: list\n",
    "        Lists of latitude and longitude minimum and maximum values.\n",
    "        The latitude extents are (-90, 90). The longitude extents are (-180, 180).\n",
    "    \"\"\"\n",
    "    ## Constants ##\n",
    "    # Resolution of the ESA CCI land classification data \n",
    "    # in degrees per pixel for latitude and longitude (300m x 300m).\n",
    "    ESA_CCI_RES = np.array([-0.002777777777778, 0.002777777777778])\n",
    "\n",
    "    def load_data_np_arr():\n",
    "        with rasterio.open(fp, driver='GTiff') as dst:\n",
    "            data_np_arr = dst.read(indexes=year_inds, window=(list(lat_px_inds), list(lon_px_inds)))\n",
    "            dst.close()\n",
    "        return data_np_arr\n",
    "    \n",
    "    assert years is not None, \"years must be specified\"\n",
    "    assert lat is not None, \"lat must be specified.\"\n",
    "    assert lon is not None, \"lon must be specified.\"\n",
    "\n",
    "    (MIN_LAT, MAX_LAT), (MIN_LON, MAX_LON) = (-90, 90), (-180, 180)\n",
    "    DEGREE_PER_PX = np.abs(ESA_CCI_RES)\n",
    "    PX_PER_DEGREE = 1/DEGREE_PER_PX # About 360.\n",
    "    years, lat, lon = np.array(years), np.array(lat), np.array(lon)\n",
    "    \n",
    "    # Select years.\n",
    "    year_inds = list(years-1991)\n",
    "    # Select area.\n",
    "    # Latitude pixel indices are handled differently than longitude because \n",
    "    # latitude indexing is from top to bottom in this dataset.\n",
    "    lat_px_inds = np.round(PX_PER_DEGREE[0]*(MAX_LAT - lat)).astype(np.int32)[::-1]\n",
    "    lon_px_inds = np.round(PX_PER_DEGREE[1]*(lon - MIN_LON)).astype(np.int32)\n",
    "    \n",
    "    data_np_arr = load_data_np_arr() # Load the data.\n",
    "    # Determine latitude and longitude coordinates.\n",
    "    lat, lon = np.arange(*lat, DEGREE_PER_PX[0])[::-1], np.arange(*lon, DEGREE_PER_PX[1])\n",
    "    # Handle too many coordinates (\"one-off\" errors).\n",
    "    lat, lon = lat[:data_np_arr.shape[1]], lon[:data_np_arr.shape[2]]\n",
    "    return xr.DataArray(data=data_np_arr, coords=[years, lat, lon], dims=['time', 'latitude', 'longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FROM-GLC Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_time import _n64_to_datetime\n",
    "from utils.data_cube_utilities.dc_load import is_dataset_empty\n",
    "\n",
    "def load_FROM_GLC_data(years=years, lat=lat_small, lon=lon_small):\n",
    "    # Only data for the first and last years should be loaded.\n",
    "    land_cls_data = {}\n",
    "    for year in years:\n",
    "        land_cls_data[year] = dc.load(product='from_glc', lat=lat, lon=lon, \n",
    "                                      measurements=['land_class'], time=[f\"{year}-01-01\", f\"{year}-12-31\"])\n",
    "    land_cls_data = xr.concat(list(land_cls_data.values()), dim='time')\n",
    "    assert not is_dataset_empty(land_cls_data), \"No data was retrieved.\"\n",
    "    land_cls_data = land_cls_data['land_class']\n",
    "    # The first and last years of the land classification data define the range of years to load Data Cube data for.\n",
    "    land_cls_years = [_n64_to_datetime(time).year for time in land_cls_data.time.values]\n",
    "    # Set the times to the years as integers.\n",
    "    land_cls_data.time.values = land_cls_years\n",
    "    return land_cls_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings ##\n",
    "data_source = \"ESA_CCI_LC\" # Can be one of [\"ESA_CCI_LC\", \"FROM-GLC\"]\n",
    "## End Settings ##\n",
    "\n",
    "data_source_load_func_map = {\n",
    "    \"ESA_CCI_LC\": load_ESA_CCI_data,\n",
    "    \"FROM-GLC\": load_FROM_GLC_data\n",
    "}\n",
    "\n",
    "land_cls_data = data_source_load_func_map[data_source]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### <span id=\"load_data_cube\">Load EO Data from the Datacube [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_time import _n64_to_datetime\n",
    "from utils.data_cube_utilities.dc_load import is_dataset_empty\n",
    "\n",
    "assert land_cls_data.count() > 0, \"No data was retrieved.\"\n",
    "# The first and last years of the land classification data define the range of years to load Data Cube data for.\n",
    "date_ranges = {year: [dt.datetime(year,1,1), dt.datetime(year,12,31)] for year in years[[0,-1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_load import match_dim_sizes, find_desired_acq_inds, \\\n",
    "                                                      reduce_on_day\n",
    "from utils.data_cube_utilities.clean_mask import landsat_qa_clean_mask, landsat_clean_mask_invalid\n",
    "from utils.data_cube_utilities.aggregate import xr_scale_res\n",
    "from utils.data_cube_utilities.sort import xarray_sortby_coord\n",
    "from utils.data_cube_utilities.dc_mosaic import create_median_mosaic\n",
    "from utils.data_cube_utilities.dc_utilities import ignore_warnings\n",
    "from utils.data_cube_utilities.vegetation import NDVI\n",
    "\n",
    "datacube_median_data = [None]*len(years) # Yearly median composites.\n",
    "datacube_ndvi_data = [None]*len(years) # Yearly average NDVI.\n",
    "year_inds_no_data = [] # A list of indices into the above lists for years with no data.\n",
    "measurements = ['red', 'green', 'blue', 'nir', 'swir1', 'swir2', 'pixel_qa']\n",
    "for year_ind, date_range in enumerate(date_ranges.values()):\n",
    "    # First, load and combine the data.\n",
    "    datasets = {}\n",
    "    clean_masks = {}\n",
    "    for platform, product in zip(platforms, products):\n",
    "        # Load the dataset.\n",
    "        dataset = dc.load(platform=platform, product=product, lat=lat_small, lon=lon_small, \n",
    "                          time=date_range, measurements=measurements)\n",
    "        if len(dataset.dims) == 0: # The dataset is empty.\n",
    "            continue\n",
    "        # Ensure the Data Cube data has the same shape as the land classification data.\n",
    "        dataset = ignore_warnings(xr_scale_res, dataset, \n",
    "                                  abs_res=(len(land_cls_data.longitude), len(land_cls_data.latitude)))\n",
    "        # Get the clean mask.\n",
    "        clean_mask = landsat_qa_clean_mask(dataset, platform) \n",
    "        clean_mask = clean_mask & (dataset[measurements[0]] != -9999) \n",
    "        clean_mask = clean_mask & landsat_clean_mask_invalid(dataset)\n",
    "        # Remove the 'pixel_qa' data variable since we have the clean mask.\n",
    "        dataset = dataset.drop('pixel_qa')    \n",
    "        # Clean the data.\n",
    "        dataset = dataset.where(clean_mask)\n",
    "        datasets[product], clean_masks[product] = dataset, clean_mask\n",
    "    del dataset, clean_mask\n",
    "    # Combine everything.\n",
    "    if len(datasets) > 0:\n",
    "        dataset = xarray_sortby_coord(xr.concat(list(datasets.values()), dim='time'), coord='time')\n",
    "        clean_mask = xarray_sortby_coord(xr.concat(list(clean_masks.values()), dim='time'), coord='time')\n",
    "        datacube_ndvi_data[year_ind] = NDVI(dataset.mean(dim=['latitude', 'longitude', 'time']))\n",
    "        datacube_median_data[year_ind] = ignore_warnings(create_median_mosaic, dataset, clean_mask)\n",
    "    else:\n",
    "        year_inds_no_data.append(year_ind)\n",
    "        dataset = xr.Dataset()\n",
    "        clean_mask = xr.DataArray(np.empty((0,), dtype=np.bool))\n",
    "    del datasets, clean_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "year_inds_with_data = np.setdiff1d(np.arange(len(years)), year_inds_no_data)\n",
    "years_with_data = years[year_inds_with_data]\n",
    "first_year_with_data, last_year_with_data = years_with_data[[0,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "if len(year_inds_no_data) > 0:\n",
    "    # If all years, the first year, or the last year are missing, the first and last years selected must be changed.\n",
    "    assert_msg_begin = \"The selected combination of platforms, products, and extents have no data \"\n",
    "    assert_msg_end = \" The first and last years with data available are {} and {}.\"\\\n",
    "                     .format(first_year_with_data, last_year_with_data)\n",
    "    # Whether the first and last selected years have data.\n",
    "    first_year_has_data, last_year_has_data = year_inds_no_data[0] != 0, year_inds_no_data[-1] != len(years)-1\n",
    "    assert len(year_inds_no_data) != len(years), assert_msg_begin + \"for any of the selected years.\" + assert_msg_end\n",
    "    assert first_year_has_data or last_year_has_data, assert_msg_begin + \"for the first and last selected years.\" + assert_msg_end\n",
    "    assert first_year_has_data, assert_msg_begin + \"for the first selected year.\" + assert_msg_end\n",
    "    assert last_year_has_data, assert_msg_begin + \"for the last selected year.\" + assert_msg_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# For years which have no data and are between the first and last years, \n",
    "# shape them as the other years, but fill with NaNs.\n",
    "median_data_first_year_avail = datacube_median_data[year_inds_with_data[0]]\n",
    "median_data_full_NaNs = xr.full_like(median_data_first_year_avail, np.nan, dtype=np.float64)\n",
    "ndvi_data_first_year_avail = datacube_ndvi_data[year_inds_with_data[0]]\n",
    "ndvi_data_NaN = xr.full_like(ndvi_data_first_year_avail, np.nan, dtype=np.float64) \n",
    "for year_ind in year_inds_no_data:\n",
    "    datacube_median_data[year_ind] = median_data_full_NaNs\n",
    "    datacube_ndvi_data[year_ind] = ndvi_data_NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datacube_ndvi_data = xr.concat(datacube_ndvi_data, dim='time')\n",
    "datacube_ndvi_data.coords['time'] = years\n",
    "datacube_median_data = xr.concat(datacube_median_data, dim='time')\n",
    "datacube_median_data.coords['time'] = years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## <span id=\"train_cls\">Train Random Forest Classifier [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### <span id='train_model'>Train Model [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "from utils.data_cube_utilities.dc_utilities import reverse_array_dict\n",
    "\n",
    "# For the feature matrix, make one row for the band values for each pixel for each year.\n",
    "datacube_median_data_arr = datacube_median_data.sel(time=[first_year, last_year]).to_array(dim='data_var').\\\n",
    "                           transpose('time', 'latitude', 'longitude', 'data_var').values\n",
    "num_data_vars = datacube_median_data_arr.shape[-1]\n",
    "\n",
    "# Encode data as 6 IPCC land categories.\n",
    "# For a label, returns all corresponding land class values.\n",
    "land_cls_values_for_label_by_data_source= {\n",
    "    # See the ECA CCI Quick User Guide for these codes and their more detailed meanings.\n",
    "    \"ESA_CCI_LC\":\n",
    "        OrderedDict([ # For a label, returns all corresponding land class values.\n",
    "            ('Forest Land', np.array([50, 60, 61, 62, 70, 71, 72, 80, 81, 82, 90, 100])),\n",
    "            ('Grassland', np.array([40, 120, 121, 122, 130, 140, 150, 152, 153])),\n",
    "            ('Cropland', np.array([10, 11, 12, 20, 30, 110])),\n",
    "            ('Wetlands', np.array([160, 170, 180])),\n",
    "            ('Settlements', np.array([190])),\n",
    "            ('Other Land', np.array([200, 201, 202, 210, 220])),\n",
    "            ('No Data', np.array([0]))\n",
    "        ]),\n",
    "    # See the classification system tables for these codes and their more detailed meanings:\n",
    "    # http://data.ess.tsinghua.edu.cn/\n",
    "    \"FROM-GLC\":\n",
    "        OrderedDict([ # For a label, returns all corresponding land class values.\n",
    "            ('Forest Land', np.array(list(range(20,30)))),\n",
    "            ('Grassland', np.array(list(range(30,50)) + list(range(70,80)))),\n",
    "            ('Cropland', np.array(list(range(10,20)))),\n",
    "            ('Wetlands', np.array(list(range(50,60)))),\n",
    "            ('Settlements', np.array(list(range(60,70)) + list(range(80, 90)))),\n",
    "            ('Other Land', np.array(list(range(90,110)))),\n",
    "            ('No Data', np.array([120]))\n",
    "        ])\n",
    "}\n",
    "\n",
    "land_cls_values_for_label = land_cls_values_for_label_by_data_source[data_source]\n",
    "\n",
    "# For a land class value, returns the corresponding label.\n",
    "land_cls_label_for_value = reverse_array_dict(land_cls_values_for_label)\n",
    "\n",
    "# Encode the data.\n",
    "land_cls_data_encoded = land_cls_data.copy()\n",
    "for i, (cls_label, values_for_label) in enumerate(land_cls_values_for_label.items()):\n",
    "    land_cls_data_encoded.values[np.isin(land_cls_data.values, values_for_label)] = i\n",
    "\n",
    "land_cls_data_arr_encoded = land_cls_data_encoded.transpose('time', 'latitude', 'longitude').values\n",
    "# Obtain a version of `land_cls_data_encoded` for visualization by \n",
    "# setting pixels without valid land class data to NaN.\n",
    "land_cls_data_encoded_vis = land_cls_data_encoded.astype(np.float32)\n",
    "land_cls_data_encoded_vis = land_cls_data_encoded_vis.where(land_cls_data_encoded_vis != 6, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the land class data to use as class labels.\n",
    "land_cls_data_arr_encoded_flat = land_cls_data_arr_encoded.flatten()\n",
    "# Select only pixels which have valid land class data.\n",
    "valid_land_cls_mask = land_cls_data_arr_encoded_flat != 6\n",
    "y_pred = np.empty_like(land_cls_data_arr_encoded_flat, dtype=np.float32)\n",
    "\n",
    "X, y = datacube_median_data_arr, land_cls_data_arr_encoded_flat\n",
    "X = X.reshape((-1, X.shape[-1]))\n",
    "# Remove corresponding entries in `X` and `y` where either has NaN values.\n",
    "no_nan_mask = (~np.isnan(X).any(axis=1)) & ~np.isnan(y)\n",
    "clean_model_data_mask = no_nan_mask & valid_land_cls_mask\n",
    "X_clean, y_clean = X[clean_model_data_mask], y[clean_model_data_mask]\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50]\n",
    "}\n",
    "num_folds = 5\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid,\n",
    "                           scoring=make_scorer(accuracy_score), \n",
    "                           cv=num_folds, n_jobs=-1)\n",
    "grid_search.fit(X_clean, y_clean)\n",
    "model, score, best_params = grid_search.best_estimator_, grid_search.best_score_, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "print(\"The best parameter set was:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### <span id='get_score_pred'>Get Model Score and Predictions [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "y_pred_clean = model.predict(X_clean)\n",
    "\n",
    "# Evaluate performance.\n",
    "print(\"Model Cross Validation Score: {0:0.6%} (Stratified {1}-fold)\".format(score, num_folds))\n",
    "print(\"Model Accuracy: {0:0.6%}\".format(accuracy_score(y_clean, y_pred_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### <span id='prepare_vis'>Prepare for Visualization [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.plotter_utils import \\\n",
    "    figure_ratio, xarray_imshow, create_discrete_color_map\n",
    "\n",
    "# Determine the class labels to use in visualizations.\n",
    "land_cls_values_for_label_copy = land_cls_values_for_label.copy()\n",
    "land_cls_values_for_label_copy.pop('No Data', None)\n",
    "classes = np.array(list(land_cls_values_for_label_copy.keys()))\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Reshape land class predictions for visualization.\n",
    "y_pred[clean_model_data_mask] = y_pred_clean\n",
    "y_pred[~clean_model_data_mask] = np.nan\n",
    "dim_lengths_shape = land_cls_data_encoded.shape\n",
    "y_pred_3D = xr.DataArray(data=y_pred.reshape(dim_lengths_shape), coords=land_cls_data_encoded.coords, \n",
    "                         dims=land_cls_data_encoded.dims, attrs=land_cls_data_encoded.attrs)\n",
    "\n",
    "# Gets figure sizes for plotting geospatial data.\n",
    "def get_figsize_geospatial(fixed_width=6, fixed_height=None, \n",
    "                           num_cols=1, num_rows=1):\n",
    "    return figure_ratio(datacube_median_data, \n",
    "                        fixed_width=fixed_width, fixed_height=fixed_height,\n",
    "                        num_cols=num_cols, num_rows=num_rows)\n",
    "\n",
    "# Get a mapping of values to labels for legend labelling.\n",
    "legend_labels = {value:label for value, label in enumerate(classes)}\n",
    "legend_values = list(legend_labels.keys())\n",
    "\n",
    "# Create a colormap for the predicted and true land classes.\n",
    "map_cmap = create_discrete_color_map((0, len(classes)-1), \n",
    "                                 colors=['red', 'yellow', 'green', 'blue', 'brown', 'black'])\n",
    "# Create a colormap for coloring the cells in the change matrix.\n",
    "change_matrix_cmap = create_discrete_color_map([-1,1], colors=[(255, 204, 204), (189, 215, 238), (198, 224, 180)])\n",
    "\n",
    "title_fontdict = dict(fontsize=20) # Title formatting\n",
    "tick_label_fmt_dict = dict(axis='both', labelsize=12) # Tick label formatting\n",
    "axis_label_fmt_dict = dict(fontsize=16) # Axis label formatting\n",
    "legend_kwargs = dict(fontsize=12, framealpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### <span id='vis_ESA_CCI'>Visualize Land Class Data for the Beginning and End of the Time Range [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.dc_rgb import rgb\n",
    "\n",
    "num_rows, num_cols = 1, 2\n",
    "figsize = get_figsize_geospatial(fixed_width=8, num_cols=num_cols, num_rows=num_rows)\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "fig, ax[0] = rgb(datacube_median_data.sel(time=first_year), fig=fig, ax=ax[0],\n",
    "                 use_data_min=True, use_data_max=True)\n",
    "ax[0].set_title(\"Median Composite for {}\".format(first_year))\n",
    "fig, ax[1] = rgb(datacube_median_data.sel(time=last_year), fig=fig, ax=ax[1],\n",
    "                 use_data_min=True, use_data_max=True)\n",
    "ax[1].set_title(\"Median Composite for {}\".format(last_year))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.plotter_utils import xarray_imshow\n",
    "\n",
    "# Show true land classes for the first and last years.\n",
    "num_rows, num_cols = 1, 2\n",
    "figsize = get_figsize_geospatial(fixed_width=8, num_cols=num_cols, num_rows=num_rows)\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "fig, ax[0], im, cbar = \\\n",
    "xarray_imshow(land_cls_data_encoded.sel(time=first_year), fig=fig, ax=ax[0],\n",
    "              use_colorbar=False, use_legend=True,\n",
    "              legend_labels=legend_labels,\n",
    "              title=\"Land Classes for {}\".format(first_year),\n",
    "              possible_plot_values=legend_values)\n",
    "ax[0].set_xlabel('Longitude')\n",
    "ax[0].set_ylabel('Latitude')\n",
    "fig, ax[1], im, cbar = \\\n",
    "    xarray_imshow(land_cls_data_encoded.sel(time=last_year), fig=fig, ax=ax[1],\n",
    "                  use_colorbar=False, use_legend=True,\n",
    "                  legend_labels=legend_labels,\n",
    "                  title=\"Land Classes for {}\".format(last_year),\n",
    "                  possible_plot_values=legend_values)\n",
    "ax[1].set_xlabel('Longitude')\n",
    "ax[1].set_ylabel('Latitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### <span id='vis_pred_true_land_cls'>Visualize Predicted Land Classes (from Model) and \"True\" Land Classes [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "**Note that white pixels in the predicted figure are due to missing or masked data, which includes regions occluded by clouds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show predicted and true land classes side-by-side for some years.\n",
    "years_to_show = first_year, last_year # Select the years you wish to examine.\n",
    "for year in years_to_show:\n",
    "    assert year in years, \\\n",
    "        \"The year {} has not been loaded. Only include years \"\\\n",
    "        \"in the variable 'years_to_show' which are also in the variable 'years'.\".format(year)\n",
    "\n",
    "num_rows, num_cols = len(years_to_show), 2\n",
    "figsize = get_figsize_geospatial(num_cols=num_cols, num_rows=num_rows)\n",
    "fig, ax = plt.subplots(num_rows, num_cols, figsize=figsize)#, dpi=std_dpi)\n",
    "for year_ind, year in enumerate(years_to_show):\n",
    "    # Show the predicted land classes on the left.\n",
    "    fig, ax[year_ind,0], im, cbar = \\\n",
    "        xarray_imshow(y_pred_3D[year_ind], fig=fig, ax=ax[year_ind,0], \n",
    "                      use_colorbar=False, use_legend=True,\n",
    "                      legend_labels=legend_labels,\n",
    "                      title=\"Predicted Land Classes for {}\".format(year),\n",
    "                      possible_plot_values=legend_values)\n",
    "    ax[year_ind, 0].set_xlabel('Longitude')\n",
    "    ax[year_ind, 0].set_ylabel('Latitude')\n",
    "    # Show the true land classes on the right.\n",
    "    fig, ax[year_ind,1], im, cbar = \\\n",
    "        xarray_imshow(land_cls_data_encoded_vis.sel(time=year), fig=fig, ax=ax[year_ind,1], \n",
    "                      use_colorbar=False, use_legend=True,\n",
    "                      legend_labels=legend_labels,\n",
    "                      title=\"Land Classes for {}\".format(year),\n",
    "                      possible_plot_values=legend_values)\n",
    "    ax[year_ind, 1].set_xlabel('Longitude')\n",
    "    ax[year_ind, 1].set_ylabel('Latitude')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## <span id=\"land_change_freq\">Visualize Land Change Frequency [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Get the number of class changes for each pixel between the first and last years.\n",
    "example_xr = land_cls_data.isel(time=0)\n",
    "num_cls_changes = xr.DataArray(data=np.zeros_like(example_xr), coords=example_xr.coords, \n",
    "                               dims=example_xr.dims, attrs=example_xr.attrs).astype(np.int8)\n",
    "for time_ind in range(1, len(land_cls_data_encoded.time)):\n",
    "    prev_classes = land_cls_data_encoded.isel(time=time_ind-1)\n",
    "    current_classes = land_cls_data_encoded.isel(time=time_ind)\n",
    "    cls_changes = (current_classes != prev_classes).astype(np.int8)\n",
    "    num_cls_changes += cls_changes\n",
    "unq_num_cls_changes = np.unique(num_cls_changes.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "pts_fmt = [None]*len(unq_num_cls_changes)\n",
    "cmap = create_discrete_color_map([min(unq_num_cls_changes),max(unq_num_cls_changes)], \n",
    "                                 pts=unq_num_cls_changes, cmap='viridis', pts_fmt=pts_fmt)\n",
    "\n",
    "figsize = get_figsize_geospatial()\n",
    "fig = plt.figure(figsize=figsize)\n",
    "fig, ax, im, cbar = \\\n",
    "    xarray_imshow(num_cls_changes, fig=fig, cbar_labels=unq_num_cls_changes, \n",
    "                  imshow_kwargs=dict(cmap=cmap), cbar_kwargs=dict(ticks=pts_fmt),\n",
    "                  title=\"# Land Class Changes from {} to {}\".format(first_year, last_year))\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## <span id=\"change_matrix\">Create Change Matrix [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "**To specify what transitions are considered positive, neutral, or negative, alter `pos_neu_neg_cng_mat` in the code cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from utils.data_cube_utilities.plotter_utils import print_matrix\n",
    "\n",
    "# Get class values for the first and last year.\n",
    "original_year_classes = land_cls_data_encoded.sel(time=first_year).\\\n",
    "                        transpose('latitude', 'longitude').values.flatten()\n",
    "final_year_classes = land_cls_data_encoded.sel(time=last_year).\\\n",
    "                     transpose('latitude', 'longitude').values.flatten()\n",
    "\n",
    "# Select only pixels which have valid land class data.\n",
    "valid_mask = (original_year_classes != 6) & (final_year_classes != 6)\n",
    "original_year_classes, final_year_classes = \\\n",
    "    original_year_classes[valid_mask], final_year_classes[valid_mask]\n",
    "\n",
    "# Get 2-tuples of (original_class, final_class), fill out the change matrix,then convert to percentages of total pixels.\n",
    "original_final_cls = np.stack([original_year_classes, final_year_classes], axis=1)\n",
    "change_matrix = np.zeros((num_classes, num_classes), dtype=np.int32)\n",
    "for orig_cls, final_cls in original_final_cls:\n",
    "    change_matrix[orig_cls, final_cls] += 1\n",
    "change_matrix = change_matrix / change_matrix.sum()\n",
    "\n",
    "# IMPORTANT: This is a matrix specifying each land class transition as positive (1 - green), \n",
    "#            neutral (0 - blue), or negative (-1 - red). Original classes are indexed by row and\n",
    "#            final classes are indexed by column.\n",
    "neg_cng_val, neu_cng_val, pos_cng_val = -1, 0, 1\n",
    "pos_neu_neg_cng_mat = np.array([\n",
    "    # Forest Land  Grassland  Cropland  Wetlands  Settlements  Other Land\n",
    "    [        0,        -1,        -1,        -1,        -1,        -1], # Forest Land\n",
    "    [        1,         0,         1,        -1,        -1,        -1], # Grassland \n",
    "    [        1,        -1,         0,        -1,        -1,        -1], # Cropland \n",
    "    [       -1,        -1,        -1,         0,        -1,        -1], # Wetlands\n",
    "    [        1,         1,         1,         1,         0,         1], # Settlements\n",
    "    [        1,         1,         1,         1,        -1,         0]  # Other Land    \n",
    "])\n",
    "\n",
    "# Create mappings of transitions to values for coloring based on a colormap.\n",
    "cls_trans_for_value = {pos_cng_val:[], neu_cng_val:[], neg_cng_val:[]}\n",
    "for i, orig_class in enumerate(classes):\n",
    "    for j, final_class in enumerate(classes):\n",
    "        pos_neu_neg_cng_val = pos_neu_neg_cng_mat[i,j]\n",
    "        cls_trans_for_value[pos_neu_neg_cng_val].append((orig_class, final_class))\n",
    "value_for_cls_trans = reverse_array_dict(cls_trans_for_value)\n",
    "\n",
    "# Create mappings of transitions to transition type labels (e.g. \"Afforestation\").\n",
    "cls_trans_for_label = {\n",
    "    \"Stable\": [('Forest Land', 'Forest Land'), ('Grassland', 'Grassland'),\n",
    "               ('Cropland', 'Cropland'), ('Wetlands', 'Wetlands'),\n",
    "               ('Settlements', 'Settlements'), ('Other Land', 'Other Land')],\n",
    "    \"Vegetation Loss\": [('Forest Land', 'Grassland'), ('Forest Land', 'Other Land'),\n",
    "                        ('Grassland', 'Other Land'), ('Cropland', 'Other Land')],\n",
    "    \"Deforestation\": [('Forest Land', 'Cropland'), ('Forest Land', 'Settlements'), \n",
    "                      ],\n",
    "    \"Inundation\": [('Forest Land', 'Wetlands'), ('Grassland', 'Wetlands'), \n",
    "                   ('Cropland', 'Wetlands')],\n",
    "    \"Afforestation\": [('Grassland', 'Forest Land'), ('Cropland', 'Forest Land'),\n",
    "                      ('Settlements', 'Forest Land'), ('Other Land', 'Forest Land')],\n",
    "    \"Agricultural Expansion\": [('Grassland', 'Cropland'), ('Settlements', 'Cropland'),\n",
    "                               ('Other Land', 'Cropland')],\n",
    "    \"Urban Expansion\": [('Grassland', 'Settlements'), ('Cropland', 'Settlements'),\n",
    "                        ('Other Land', 'Settlements')],\n",
    "    \"Withdrawl of Agriculture\": [('Cropland', 'Grassland')],\n",
    "    \"Woody Encroachment\": [('Wetlands', 'Forest Land')],\n",
    "    \"Wetland Drainage\": [('Wetlands', 'Grassland'), ('Wetlands', 'Cropland'), \n",
    "                         ('Wetlands', 'Settlements'), ('Wetlands', 'Other Land')],\n",
    "    \"Vegetation Establishment\": [('Settlements', 'Grassland'), ('Other Land', 'Grassland')],\n",
    "    \"Wetland Establishment\": [('Settlements', 'Wetlands'), ('Other Land', 'Wetlands')],\n",
    "    \"Withdrawl of Settlements\": [('Settlements', 'Other Land')],\n",
    "}\n",
    "cls_label_for_trans = reverse_array_dict(cls_trans_for_label)\n",
    "\n",
    "# Create the cell value matrix (used to color cells).\n",
    "cell_value_mtx = np.empty_like(change_matrix)\n",
    "for i, cls_label1 in enumerate(classes):\n",
    "    for j, cls_label2 in enumerate(classes):\n",
    "        cell_value_mtx[i,j] = value_for_cls_trans[(cls_label1, cls_label2)]\n",
    "        \n",
    "# Create the cell label matrix.\n",
    "cell_label_mtx = np.empty_like(change_matrix, dtype=object)\n",
    "for i, cls_label1 in enumerate(classes):\n",
    "    for j, cls_label2 in enumerate(classes):\n",
    "        cell_label_mtx[i,j] = \\\n",
    "            \"{0:.2%}\\n{1}\".format(change_matrix[i,j], \n",
    "                                  cls_label_for_trans[(cls_label1, cls_label2)].replace(\" \", \"\\n\"))\n",
    "\n",
    "# Show the change matrix\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig, ax = print_matrix(cell_value_mtx=cell_value_mtx, cell_label_mtx=cell_label_mtx, \n",
    "                       row_labels=classes, col_labels=classes, cmap=change_matrix_cmap, \n",
    "                       cell_val_fmt='s', annot_kwargs=dict(size=14), \n",
    "                       x_axis_tick_kwargs=dict(rotation=0), x_axis_ticks_position='top', fig=fig)\n",
    "ax.yaxis.set_label_position('left')\n",
    "plt.ylabel('Original Class', fontsize=16)\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xlabel('Final Class', fontsize=16)\n",
    "plt.title('Change Matrix between {} and {}'.format(first_year, last_year), fontsize=18, pad=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## <span id=\"peak_ndvi_within_class\">Calculate Peak NDVI Within Classes [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class values for the first and last year.\n",
    "datacube_original_year_data_arr = NDVI(datacube_median_data.sel(time=first_year)[['nir', 'red']]).\\\n",
    "                                  transpose('latitude', 'longitude').values.flatten()[valid_mask]\n",
    "datacube_final_year_data_arr = NDVI(datacube_median_data.sel(time=last_year)[['nir', 'red']]).\\\n",
    "                               transpose('latitude', 'longitude').values.flatten()[valid_mask]\n",
    "    \n",
    "# Calculate the maximum NDVI for all pixels that are the same class in the first and final years.\n",
    "max_ndvi_per_class = np.zeros(num_classes)\n",
    "for i in range(num_classes):\n",
    "    stable_mask = np.all(original_final_cls == i, axis=1)\n",
    "    class_ndvi_vals = datacube_original_year_data_arr[stable_mask]\n",
    "    max_ndvi_per_class[i] = np.nanmax(class_ndvi_vals) if len(class_ndvi_vals) > 0 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16 # Set the font size for this plot..\n",
    "\n",
    "width = min(18, 3*num_classes)\n",
    "height = width/num_classes\n",
    "print_matrix(max_ndvi_per_class.reshape(1,-1), show_row_labels=False, \n",
    "             col_labels=classes, cmap='Greens', x_axis_tick_kwargs=dict(rotation=0), \n",
    "             x_axis_ticks_position='top', heatmap_kwargs=dict(vmin=-1, vmax=1), \n",
    "             fig_kwargs=dict(figsize=(width,height)))\n",
    "plt.title('Peak NDVI Within Class (no class change between {} and {})'.format(first_year, last_year), \n",
    "          fontsize=14, pad=30)\n",
    "plt.show()\n",
    "\n",
    "plt.rcParams['font.size'] = default_fontsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## <span id=\"ndvi_trend\">Determine NDVI Trend [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### <span id='thiel_sen_reg'>Fit a Thiel-Sen Regressor to the Mean NDVI Across Time [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "regressor_y = datacube_ndvi_data.values\n",
    "regressor_time_pre_fmt = datacube_ndvi_data.time.values\n",
    "regressor_time = regressor_time_pre_fmt.reshape(-1,1)\n",
    "\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "regressor = TheilSenRegressor().fit(regressor_time, regressor_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### <span id='mann_kendall_test'>Run the Mann-Kendall Test on the Mean NDVI Across Time to Determine the Trend [&#9652;](#top)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def mann_kendall_test(t, x):\n",
    "    \"\"\"\n",
    "    Runs the Mann-Kendall test to determine a trend in time series data.\n",
    "    \n",
    "    This function is a modified form of the function defined here:\n",
    "        https://up-rs-esp.github.io/mkt/_modules/mkt.html#test\n",
    "    Documentation used includes: \n",
    "        https://up-rs-esp.github.io/mkt/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : 1D numpy.ndarray\n",
    "        Array of the time points of measurements.\n",
    "    x : 1D numpy.ndarray\n",
    "        Array containing the measurements corresponding to entries of 't'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Z_mk: float\n",
    "        The Z-score statistic for the Mann-Kendall test.\n",
    "    \"\"\"\n",
    "    # Calculate signs of all possible differences x_j - x_k, where j > k.\n",
    "    n = len(t)\n",
    "    sgn = np.full(int((n-1)*n/2), 10)\n",
    "    ind = 0\n",
    "    for j in range(n-1):\n",
    "        sgn[ind:ind+(n-1-j)] = np.sign(x[j+1:] - x[j])\n",
    "        ind += n-1-j\n",
    "    \n",
    "    # Calculate the mean of the signs of the differences.\n",
    "    S_mean = sgn.sum()\n",
    "    \n",
    "    # Calculate the variance of the signs of the differences.\n",
    "    unique_x = np.unique(x)\n",
    "    num_tie_groups = len(unique_x)\n",
    "    tie_coef = 0\n",
    "    if num_tie_groups < len(x): # Calculate the tie group coefficient.\n",
    "        for tie_ind in range(num_tie_groups):\n",
    "            tie_val = unique_x[tie_ind]\n",
    "            n_t_g = len(x[x==tie_val]) # The number of ties for this tie group.\n",
    "            tie_coef += n_t_g*(n_t_g-1)*(2*n_t_g+5)\n",
    "    varS = (n*(n-1)*(2*n+5) - tie_coef)/18\n",
    "    \n",
    "    # Compute the Z-score based on above estimated mean and variance\n",
    "    if S_mean > 0:\n",
    "        Z_mk = (S_mean-1)/np.sqrt(varS)\n",
    "    if S_mean == 0:\n",
    "        Z_mk = 0\n",
    "    if S_mean < 0:\n",
    "        Z_mk = (S_mean+1)/np.sqrt(varS)\n",
    "    \n",
    "    return Z_mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "z_score = mann_kendall_test(regressor_time_pre_fmt, regressor_y)\n",
    "\n",
    "z_score_label = None\n",
    "if z_score < -1.96:\n",
    "    z_score_label = \"Potential Degradation\"\n",
    "if (-1.96 < z_score) and (z_score < 1.96):\n",
    "    z_score_label = \"No Significant Change\"\n",
    "if 1.96 < z_score:\n",
    "    z_score_label = \"Potential Improvement\"\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(regressor_time_pre_fmt, regressor_y, label='Original Data')\n",
    "plt.plot(regressor_time_pre_fmt, regressor.predict(regressor_time), label='Mann-Kendall Regression')\n",
    "plt.legend()\n",
    "plt.title(\"NDVI Trend - Mann-Kendall Z score: {:.3f} - {}\".format(z_score, z_score_label))\n",
    "plt.xticks(regressor_time_pre_fmt)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"NDVI\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
